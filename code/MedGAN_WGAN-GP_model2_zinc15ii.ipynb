{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGAN\n",
    "\n",
    "Model 2 | Subset ZINC15-II\n",
    "\n",
    "Based on the principles of Wasserstein Generative Adversarial Networks and Graph Convolutional Networks, MedGAN generates new quinoline-scaffold molecules from molecular graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem.rdmolops import AddHs\n",
    "from rdkit.Chem.rdmolops import GetMolFrags\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem, Draw, Descriptors\n",
    "from rdkit.Chem import AtomValenceException\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
    "import rdkit.RDLogger as rdl\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from PIL import Image\n",
    "import gzip\n",
    "import pickle\n",
    "import psutil\n",
    "import pygraphviz as pgv\n",
    "import time\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logger = rdl.logger()\n",
    "logger.setLevel(rdl.ERROR)\n",
    "logger.setLevel(rdl.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Filtering and Sampling of Quinoline Molecules\n",
    "\n",
    "The following section is dedicated to loading and processing quinoline molecule data from the ZINC15 dataset. Initially, the script checks whether a pre-filtered dataset (non_duplicate_filtered_quinolines_zinc15_50atoms.csv) exists. If it doesn't, the original dataset (non_duplicate_quinolines_zinc15.csv) is loaded and molecules are filtered based on specific criteria:\n",
    "\n",
    "Only molecules containing the atoms - Carbon (C), Nitrogen (N), Oxygen (O), Hydrogen (H), Fluorine (F), Sulfur (S), and Chlorine (Cl) are retained.\n",
    "    \n",
    "Molecules having up to a maximum of 50 atoms, including hydrogen, are considered.\n",
    "\n",
    "Post filtering, duplicates are removed, and the processed data is saved for future use. If the filtered dataset already exists, it is directly loaded.\n",
    "\n",
    "To ensure consistent results and manageable data sizes, a subsample of 1,000,000 molecules is randomly chosen from this dataset. This subsample is then saved for subsequent analyses. Lastly, as a quick verification, one molecule is printed to visualize its structure and compute its count of heavy atoms.\n",
    "\n",
    "Note: for Cloud Ocean run, the sample was reduced to 100,000 molecules due to memory restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filtered csv or save a new one\n",
    "csv_path = \"../data/non_duplicate_quinolines_zinc15.csv\"\n",
    "filtered_csv_path = \"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\"\n",
    "\n",
    "allowed_atoms = {'C', 'N', 'O', 'H', 'F', 'S', 'Cl'}\n",
    "max_atoms = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def has_allowed_atoms(mol, allowed_atoms):\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() not in allowed_atoms:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def total_atoms(mol):\n",
    "    return sum(atom.GetTotalNumHs() + 1 for atom in mol.GetAtoms())\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    molecule = Chem.AddHs(molecule)\n",
    "    if (molecule is not None\n",
    "        and '*' not in smiles\n",
    "        and has_allowed_atoms(molecule, allowed_atoms)\n",
    "        and total_atoms(molecule) <= max_atoms):\n",
    "        return smiles\n",
    "    return None\n",
    "\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If filtered CSV doesn't exist, read the original CSV, apply the filter, and save the filtered data\n",
    "if not os.path.isfile(filtered_csv_path):\n",
    "    # Read CSV file\n",
    "    data = pd.read_csv(csv_path, usecols=lambda col: col != 'index', header=None)\n",
    "\n",
    "    valid_smiles = []\n",
    "    for smiles in tqdm(data[0]):\n",
    "        try:\n",
    "            result = is_valid_smiles(smiles)\n",
    "            if result is not None:\n",
    "                valid_smiles.append(result)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing item:\", e)\n",
    "\n",
    "    # Remove duplicates\n",
    "    valid_smiles = list(set(valid_smiles))\n",
    "\n",
    "    # Create DataFrame with valid SMILES\n",
    "    data = pd.DataFrame(valid_smiles, columns=['smiles'])\n",
    "    \n",
    "    # Save the filtered data to a CSV file\n",
    "    data.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "# If filtered CSV exists, just load it\n",
    "else:\n",
    "    data = pd.read_csv(filtered_csv_path)\n",
    "\n",
    "data = data.sample(n=100000, random_state=1, replace=False)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the subsample to a CSV file\n",
    "data.to_csv(\"../data/data_zinc15_subset-ii/quinolines_filtered_subsample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the dataset\n",
    "\n",
    "smiles_test = data['smiles'][100]\n",
    "print(\"SMILES:\", smiles_test)\n",
    "molecule = Chem.MolFromSmiles(smiles_test)\n",
    "molecule = Chem.AddHs(molecule)\n",
    "print(\"Num heavy atoms:\", molecule.GetNumHeavyAtoms())\n",
    "molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SMILEs to Graph representation\n",
    "\n",
    "This code provides a suite of tools for representing molecules as graphs and vice versa. The main components are:\n",
    "\n",
    "- Mappings: Dictionaries (atom_mapping, bond_mapping, charge_mapping) are used to translate atom and bond types between their string names and numerical indices.\n",
    "Conversion Functions:\n",
    "\n",
    "- smiles_to_graph(): Converts SMILES strings into graph matrices.\n",
    "\n",
    "- graph_to_molecule(): Reconstructs molecules from their graph matrices.\n",
    "\n",
    "- graph_to_networkx(): Transforms graph matrices into NetworkX graph objects for visualization or algorithmic analysis.\n",
    "\n",
    "- Visualization: plot_graph() visually represents a molecule using the NetworkX graph, with atoms and bonds color-coded.\n",
    "\n",
    "- Data Preprocessing: Molecular data is chunked and processed in parallel to convert batches of SMILES strings into graph representations, which are then saved as compressed files for efficiency.\n",
    "\n",
    "If molecules are already converted to graphs and saved, will be loaded from compressed files.\n",
    "\n",
    "Overall, these tools facilitate the transition between chemical molecular structures and their graph representations, offering a foundation for graph-based molecular analyses or neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_mapping = {\n",
    "    \"C\": 0,\n",
    "    0: \"C\",\n",
    "    \"N\": 1,\n",
    "    1: \"N\",\n",
    "    \"O\": 2,\n",
    "    2: \"O\",\n",
    "    \"H\": 3,\n",
    "    3: \"H\",\n",
    "    \"F\": 4,\n",
    "    4: \"F\",\n",
    "    \"S\": 5,\n",
    "    5: \"S\",\n",
    "    \"Cl\": 6,\n",
    "    6: \"Cl\"\n",
    "}\n",
    "\n",
    "bond_mapping = {\n",
    "    \"SINGLE\": 0,\n",
    "    0: Chem.BondType.SINGLE,\n",
    "    \"DOUBLE\": 1,\n",
    "    1: Chem.BondType.DOUBLE,\n",
    "    \"TRIPLE\": 2,\n",
    "    2: Chem.BondType.TRIPLE,\n",
    "    \"AROMATIC\": 3,\n",
    "    3: Chem.BondType.AROMATIC,\n",
    "}\n",
    "\n",
    "NUM_ATOMS = 50\n",
    "ATOM_DIM = 7 + 1\n",
    "BOND_DIM = 4 + 1\n",
    "LATENT_DIM = 64\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    # Converts SMILES to molecule object\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    molecule = Chem.AddHs(molecule)\n",
    "\n",
    "    # Initialize adjacency and feature tensor\n",
    "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
    "\n",
    "    # Loop over each atom in molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        i = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
    "        # Loop over one-hop neighbors\n",
    "        for neighbor in atom.GetNeighbors():\n",
    "            j = neighbor.GetIdx()\n",
    "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
    "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
    "\n",
    "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
    "    # Notice: channels-first\n",
    "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
    "\n",
    "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
    "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
    "\n",
    "    return adjacency, features\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    # Unpack graph\n",
    "    adjacency, features = graph\n",
    "\n",
    "    # RWMol is a molecule object intended to be edited\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Remove \"no atoms\" & atoms with no bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    # Add atoms to molecule\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    # Add bonds between atoms in molecule; based on the upper triangles of the [symmetric] adjacency tensor\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule\n",
    "    try:\n",
    "        # Attempt to sanitize the molecule\n",
    "        flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "\n",
    "        # Check the flag returned by the sanitization\n",
    "        if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "            return None\n",
    "\n",
    "    except AtomValenceException as e:\n",
    "        # If an AtomValenceException error occurred, print the error and return None\n",
    "        print(f\"AtomValenceException during molecule sanitization: {e}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # If any other unexpected error occurred, print the error and return None\n",
    "        print(f\"Unexpected error during molecule sanitization: {e}\")\n",
    "        return None\n",
    "\n",
    "    # If sanitization was successful, return the molecule\n",
    "    return molecule\n",
    "\n",
    "def graph_to_networkx(adjacency, features):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with atom type as a property\n",
    "    for i, atom_type_idx in enumerate(tf.keras.backend.eval(tf.argmax(features, axis=1))):\n",
    "        if atom_type_idx != ATOM_DIM - 1:\n",
    "            atom_type = atom_mapping[atom_type_idx]\n",
    "            G.add_node(i, atom_type=atom_type)\n",
    "\n",
    "    # Convert adjacency matrices to NumPy arrays if input is a TensorFlow tensor\n",
    "    adjacency = [matrix.numpy() if isinstance(matrix, tf.Tensor) else matrix for matrix in adjacency]\n",
    "\n",
    "    # Add edges with bond type as a property\n",
    "    for bond_type, adjacency_matrix in enumerate(adjacency):\n",
    "        if bond_type != BOND_DIM - 1:\n",
    "            bond_type_str = bond_mapping[bond_type]\n",
    "            edge_indices = np.where(adjacency_matrix > 0)\n",
    "            edges = list(zip(*edge_indices))\n",
    "            for edge in edges:\n",
    "                G.add_edge(edge[0], edge[1], bond_type=bond_type_str)\n",
    "\n",
    "    return G\n",
    "\n",
    "def plot_graph(G, title=''):\n",
    "    atom_colors = {\n",
    "        \"C\": \"blue\",\n",
    "        \"O\": \"red\",\n",
    "        \"N\": \"yellow\",\n",
    "        \"H\": \"grey\",\n",
    "        \"F\": \"pink\",\n",
    "        \"S\": \"orange\",\n",
    "        \"Cl\": \"green\"\n",
    "    }\n",
    "\n",
    "    bond_colors = {\n",
    "        \"SINGLE\": \"black\",\n",
    "        \"DOUBLE\": \"red\",\n",
    "        \"TRIPLE\": \"blue\",\n",
    "        \"AROMATIC\": \"purple\"\n",
    "    }\n",
    "\n",
    "    node_colors = [atom_colors.get(G.nodes[node][\"atom_type\"], \"purple\") for node in G.nodes()]\n",
    "    edge_colors = [bond_colors.get(G.edges[edge][\"bond_type\"], \"black\") for edge in G.edges()]\n",
    "\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=800, edge_color=edge_colors, ax=ax1)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): d[\"bond_type\"] for u, v, d in G.edges(data=True)}, ax=ax1)\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Add legend for atoms\n",
    "    atom_legend_handles = [\n",
    "        mlines.Line2D([], [], color=color, marker=\"o\", linewidth=0, markersize=8, label=atom_type)\n",
    "        for atom_type, color in atom_colors.items()\n",
    "    ]\n",
    "    ax1.legend(handles=atom_legend_handles, loc=\"lower right\")\n",
    "\n",
    "    # Create and plot the atom type table\n",
    "    atom_data = {\"Atom Number\": list(G.nodes()), \"Atom Type\": [G.nodes[node][\"atom_type\"] for node in G.nodes()]}\n",
    "    atom_df = pd.DataFrame(atom_data)\n",
    "    atom_table = ax2.table(cellText=atom_df.values, colLabels=atom_df.columns, loc='center')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"../data/data_zinc15_subset-ii/\"\n",
    "chunk_size = 100000\n",
    "\n",
    "def process_smiles(smiles):\n",
    "    adjacency, features = smiles_to_graph(smiles)\n",
    "    return (adjacency, features)\n",
    "\n",
    "num_chunks = len(data) // chunk_size\n",
    "#print(num_chunks)\n",
    "\n",
    "def process_chunk(i):\n",
    "    adjacency_tensor_file = os.path.join(save_folder, f'adjacency_tensor_{i}.npz')\n",
    "    feature_tensor_file = os.path.join(save_folder, f'feature_tensor_{i}.npz')\n",
    "\n",
    "    if not os.path.isfile(adjacency_tensor_file) or not os.path.isfile(feature_tensor_file):\n",
    "        # If tensors do not exist, process the chunk data and create tensors\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        chunk = data['smiles'][start:end]\n",
    "        \n",
    "        # Initialize Pool with the number of available CPUs\n",
    "        with Pool(cpu_count()) as p:\n",
    "            processed_chunk_data = list(tqdm(p.imap(process_smiles, chunk), total=len(chunk), desc=f'Processing chunk {i}'))\n",
    "\n",
    "        # Convert loaded data to adjacency and feature tensors\n",
    "        adjacency_tensor, feature_tensor = zip(*processed_chunk_data)\n",
    "        adjacency_tensor = np.array(adjacency_tensor)\n",
    "        feature_tensor = np.array(feature_tensor)\n",
    "\n",
    "        # Save tensors to disk (compressed)\n",
    "        np.savez_compressed(adjacency_tensor_file, adjacency_tensor)\n",
    "        np.savez_compressed(feature_tensor_file, feature_tensor)\n",
    "\n",
    "        print(f\"Processed and saved tensors from chunk {i}\")\n",
    "        # Explicitly delete the chunk data to free up memory\n",
    "        del processed_chunk_data\n",
    "    else:\n",
    "        # If tensors exist, load them (decompressed)\n",
    "        adjacency_tensor = np.load(adjacency_tensor_file)['arr_0']\n",
    "        feature_tensor = np.load(feature_tensor_file)['arr_0']\n",
    "        \n",
    "        print(f\"Loaded tensors from chunk {i}\")\n",
    "\n",
    "    return adjacency_tensor, feature_tensor\n",
    "\n",
    "adjacency_tensors = []\n",
    "feature_tensors = []\n",
    "\n",
    "# Process each chunk individually after loading\n",
    "for i in tqdm(range(num_chunks), desc='Loading data chunks'):\n",
    "    adjacency_tensor, feature_tensor = process_chunk(i)\n",
    "    adjacency_tensors.append(adjacency_tensor)\n",
    "    feature_tensors.append(feature_tensor)\n",
    "\n",
    "adjacency_tensor = np.concatenate(adjacency_tensors)\n",
    "feature_tensor = np.concatenate(feature_tensors)\n",
    "\n",
    "print(\"adjacency_tensor.shape =\", adjacency_tensor.shape)\n",
    "print(\"feature_tensor.shape =\", feature_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN R-GCN implementation\n",
    "\n",
    "This code provides a detailed walkthrough of the implementation of WGAN R-GCN using the Keras library. We break down the components into two main sections: the Graph Generator and the Graph Discriminator.\n",
    "\n",
    "The Graph Generator function, GraphGenerator, is responsible for creating a graph representation of a molecule given latent space inputs.\n",
    "\n",
    "The Graph Discriminator function, GraphDiscriminator, assesses the \"realness\" of a molecule graph. It uses graph convolution layers to process the adjacency and feature matrices, and provides a scalar output representing the authenticity of the input graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphGenerator(\n",
    "    dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape,\n",
    "):\n",
    "    z = keras.layers.Input(shape=(LATENT_DIM,))\n",
    "    # Propagate through one or more densely connected layers\n",
    "    x = z\n",
    "    for units in dense_units:\n",
    "        #x = BatchNormalization()(x) # Add Batch Normalization layer\n",
    "        x = keras.layers.Dense(units)(x)\n",
    "        x = keras.layers.LeakyReLU(alpha=0.01)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
    "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
    "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
    "    # Symmetrify tensors in the last two dimensions\n",
    "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
    "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
    "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
    "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
    "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
    "\n",
    "    return keras.Model(inputs=z, outputs=[x_adjacency, x_features], name=\"Generator\")\n",
    "\n",
    "generator = GraphGenerator(\n",
    "    dense_units=[128, 256, 512],\n",
    "    dropout_rate=0.50,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "generator.summary()\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('generator_summary.txt', 'w') as f:\n",
    "    generator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Save the model structure as an image\n",
    "plot_model(generator, to_file='generator_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalGraphConvLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=128,  # 128\n",
    "        activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(bond_dim, atom_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            name=\"W\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(bond_dim, 1, self.units),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                trainable=True,\n",
    "                name=\"b\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        adjacency, features = inputs\n",
    "        # Aggregate information from neighbors\n",
    "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
    "        # Apply linear transformation\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        # Reduce bond types dim\n",
    "        x_reduced = tf.reduce_sum(x, axis=1)\n",
    "        # Apply non-linear transformation\n",
    "        return self.activation(x_reduced)\n",
    "\n",
    "\n",
    "def GraphDiscriminator(\n",
    "    gconv_units, dense_units, dropout_rate, adjacency_shape, feature_shape\n",
    "):\n",
    "\n",
    "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
    "    features = keras.layers.Input(shape=feature_shape)\n",
    "\n",
    "    # Propagate through one or more graph convolutional layers\n",
    "    features_transformed = features\n",
    "    for units in gconv_units:\n",
    "        features_transformed = RelationalGraphConvLayer(units)(\n",
    "            [adjacency, features_transformed]\n",
    "        )\n",
    "\n",
    "    # Reduce 2-D representation of molecule to 1-D\n",
    "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
    "\n",
    "    # Propagate through one or more densely connected layers\n",
    "    for units in dense_units:\n",
    "        #x = BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(units)(x)\n",
    "        x = keras.layers.LeakyReLU(alpha=0.01)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # For each molecule, output a single scalar value expressing the\n",
    "    # \"realness\" of the inputted molecule\n",
    "    x_out = keras.layers.Dense(1, dtype=\"float32\")(x)\n",
    "\n",
    "    return keras.Model(inputs=[adjacency, features], outputs=x_out)\n",
    "\n",
    "discriminator = GraphDiscriminator(\n",
    "    gconv_units= [128, 128, 128, 128],  # [512, 512, 512, 512],\n",
    "    dense_units= [512, 512],\n",
    "    dropout_rate=0.50,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "discriminator.summary()\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('discriminator_summary.txt', 'w') as f:\n",
    "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Save the model structure as an image\n",
    "plot_model(discriminator, to_file='discriminator_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network for Graphs\n",
    "\n",
    "The GraphWGAN class implements a Generative Adversarial Network (GAN) to generate graphs. This GAN leverages Wasserstein distance with gradient penalty for training stability.\n",
    "\n",
    "The train_step method trains the GAN for one step. This involves training the discriminator to differentiate between real and generated graphs, and training the generator to produce graphs that the discriminator cannot differentiate from real graphs.\n",
    "\n",
    "After training, you can visualize the generated graphs using the plot_generated_graph method. It visualizes the graphs with different node and edge colors according to the atom type and bond type, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphWGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        discriminator_steps=1,\n",
    "        generator_steps=1,\n",
    "        gp_weight=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator_steps = discriminator_steps\n",
    "        self.generator_steps = generator_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.latent_dim = self.generator.input_shape[-1]\n",
    "        self.epoch = 0\n",
    "        self.num_samples = 1\n",
    "        self.metric_wgan_gen_loss = keras.metrics.Mean(name=\"wgan_gen_loss\")\n",
    "\n",
    "    def compile(self, optimizer_generator, optimizer_discriminator, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer_generator = optimizer_generator\n",
    "        self.optimizer_discriminator = optimizer_discriminator\n",
    "        self.metric_generator = keras.metrics.Mean(name=\"loss_gen\")\n",
    "        self.metric_discriminator = keras.metrics.Mean(name=\"loss_dis\")\n",
    "\n",
    "    # Code to train\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        if isinstance(inputs[0], tuple):\n",
    "            inputs = inputs[0]\n",
    "\n",
    "        graph_real = inputs\n",
    "        self.batch_size = tf.shape(inputs[0])[0]\n",
    "\n",
    "        # Train the discriminator for one or more steps\n",
    "        for _ in range(self.discriminator_steps):\n",
    "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                graph_generated = self.generator(z, training=True)\n",
    "                loss = self._loss_discriminator(graph_real, graph_generated)\n",
    "            grads = tape.gradient(loss, self.discriminator.trainable_weights)\n",
    "            self.optimizer_discriminator.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "            self.metric_discriminator.update_state(loss)\n",
    "\n",
    "        # Train the generator for one or more steps\n",
    "        for _ in range(self.generator_steps):\n",
    "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                graph_generated = self.generator(z, training=True)\n",
    "                loss_wgan_generator = self._loss_generator(graph_generated)\n",
    "                self.metric_wgan_gen_loss.update_state(loss_wgan_generator)\n",
    "            grads = tape.gradient(loss_wgan_generator, self.generator.trainable_weights)\n",
    "            self.optimizer_generator.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "            self.metric_generator.update_state(loss_wgan_generator)\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_per_epoch = end_time - start_time\n",
    "\n",
    "        cpu_usage = get_cpu_usage()\n",
    "\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs['time'] = time_per_epoch\n",
    "        logs['cpu_usage'] = cpu_usage\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def _loss_discriminator(self, graph_real, graph_generated):\n",
    "        logits_real = self.discriminator(graph_real, training=True)\n",
    "        logits_generated = self.discriminator(graph_generated, training=True)\n",
    "        loss = tf.reduce_mean(logits_generated) - tf.reduce_mean(logits_real)\n",
    "        loss_gp = self._gradient_penalty(graph_real, graph_generated)\n",
    "        return loss + loss_gp * self.gp_weight\n",
    "\n",
    "    def _loss_generator(self, graph_generated):\n",
    "        logits_generated = self.discriminator(graph_generated, training=True)\n",
    "        return -tf.reduce_mean(logits_generated)\n",
    "\n",
    "    def _gradient_penalty(self, graph_real, graph_generated):\n",
    "        # Unpack graphs\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
    "        alpha = tf.random.uniform([self.batch_size])\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
    "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
    "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
    "\n",
    "        # Compute the logits of interpolated graphs\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adjacency_interp)\n",
    "            tape.watch(features_interp)\n",
    "            logits = self.discriminator(\n",
    "                [adjacency_interp, features_interp], training=True\n",
    "            )\n",
    "\n",
    "        # Compute the gradients with respect to the interpolated graphs\n",
    "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
    "        # Compute the gradient penalty\n",
    "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
    "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
    "        return tf.reduce_mean(\n",
    "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
    "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
    "        )\n",
    "\n",
    "    def save_model(self, folder_path=\"training_models_model2_zinc15_ii/WGAN\"):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        self.generator.save(os.path.join(folder_path, \"generator\"))\n",
    "        self.discriminator.save(os.path.join(folder_path, \"discriminator\"))\n",
    "\n",
    "    def load_model(self, folder_path=\"training_models_model2_zinc15_ii/WGAN\"):\n",
    "        self.generator = keras.models.load_model(os.path.join(folder_path, \"generator\"))\n",
    "        self.discriminator = keras.models.load_model(os.path.join(folder_path, \"discriminator\"))\n",
    "    \n",
    "    def plot_generated_graph(self, num_samples, epoch):\n",
    "        z = tf.random.normal((num_samples, LATENT_DIM))\n",
    "        graph = self.generator.predict(z)\n",
    "\n",
    "        adjacency = tf.argmax(graph[0], axis=1)\n",
    "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "        features = tf.argmax(graph[1], axis=2)\n",
    "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "\n",
    "        networkx_graphs = [\n",
    "            graph_to_networkx(adjacency[i].numpy(), features[i].numpy())\n",
    "            for i in range(num_samples)\n",
    "        ]\n",
    "\n",
    "        output_dir = \"output_graphs_model2_zinc15_ii\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for i, G in enumerate(networkx_graphs):\n",
    "\n",
    "            atom_colors = {\n",
    "                \"C\": \"blue\",\n",
    "                \"O\": \"red\",\n",
    "                \"N\": \"yellow\",\n",
    "                \"H\": \"grey\",\n",
    "                \"F\": \"pink\",\n",
    "                \"S\": \"orange\",\n",
    "                \"Cl\": \"green\"\n",
    "            }\n",
    "\n",
    "            bond_colors = {\n",
    "                \"SINGLE\": \"black\",\n",
    "                \"DOUBLE\": \"red\",\n",
    "                \"TRIPLE\": \"blue\",\n",
    "                \"AROMATIC\": \"purple\"\n",
    "            }\n",
    "\n",
    "            node_colors = [atom_colors.get(G.nodes[node].get(\"atom_type\", None), \"purple\") for node in G.nodes()]\n",
    "            edge_colors = [bond_colors.get(G.edges[edge].get(\"bond_type\", None), \"black\") for edge in G.edges()]\n",
    "\n",
    "            pos = nx.kamada_kawai_layout(G)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12))\n",
    "            nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=300, edge_color=edge_colors, ax=ax1)\n",
    "            nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): d[\"bond_type\"] for u, v, d in G.edges(data=True)}, ax=ax1)\n",
    "            ax1.set_title(f\"Generated Graph {i + 1}\")\n",
    "\n",
    "            # Add legend for atoms\n",
    "            atom_legend_handles = [\n",
    "                mlines.Line2D([], [], color=color, marker=\"o\", linewidth=1, markersize=8, label=atom_type)\n",
    "                for atom_type, color in atom_colors.items()\n",
    "            ]\n",
    "            ax1.legend(handles=atom_legend_handles, loc=\"lower right\")\n",
    "\n",
    "            # Create and plot the atom type table\n",
    "            #atom_data = {\"Atom Number\": list(G.nodes()), \"Atom Type\": [G.nodes[node].get(\"atom_type\", \"Unknown\") for node in G.nodes()]}\n",
    "            #atom_df = pd.DataFrame(atom_data)\n",
    "            #atom_table = ax2.table(cellText=atom_df.values, colLabels=atom_df.columns, loc='center')\n",
    "            ax2.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            filename = f\"generated_graph_epoch_{epoch}_sample_{i + 1}.png\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            fig.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training metrics\n",
    "\n",
    "This code is a detailed implementation of logging various metrics on Tensorboard related to the progress and performance of a Generative Adversarial Network (GAN) designed to generate molecular structures. It computes metrics such as validity, uniqueness, novelty, quinoline, and scaffold similarity. But also training performance on weights and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full quinoline dataset\n",
    "quinolines_df = pd.read_csv(\"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\")\n",
    "full_quinolines_smiles = set(quinolines_df['smiles'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_logdir, original_dataset, full_dataset, num_samples):\n",
    "        super().__init__()\n",
    "        self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.writer = tf.summary.create_file_writer(tensorboard_logdir)\n",
    "        self.original_smiles = set(original_dataset)\n",
    "        self.full_quinolines_smiles = set(full_dataset)\n",
    "        self.num_samples = num_samples\n",
    "        self.quinoline_scaffold = Chem.MolFromSmiles(\"n1cccc2ccccc12\")\n",
    "\n",
    "    def mol_sample(self, generator, batch_size):\n",
    "        z = tf.random.normal((batch_size, LATENT_DIM))\n",
    "        graph = generator.predict(z)\n",
    "        adjacency = tf.argmax(graph[0], axis=1)\n",
    "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "        features = tf.argmax(graph[1], axis=2)\n",
    "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "        molecules = []\n",
    "        none_counter = 0\n",
    "        for i in tqdm(range(batch_size), desc=\"Generating molecules\"):\n",
    "            try:\n",
    "                mol = graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "                molecules.append(mol)\n",
    "            except AtomValenceException:\n",
    "                molecules.append(None)\n",
    "                none_counter += 1  # Increment the counter\n",
    "        return molecules, none_counter  # Return the counter\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:  # Just trace the graph once, at the beginning\n",
    "            tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        with self.writer.as_default():\n",
    "            # Scalars\n",
    "            tf.summary.scalar(\"generator_loss\", logs.get(\"loss_gen\"), step=epoch)\n",
    "            tf.summary.scalar(\"discriminator_loss\", logs.get(\"loss_dis\"), step=epoch)\n",
    "\n",
    "            # Time per epoch\n",
    "            if 'time' in logs:\n",
    "                tf.summary.scalar(\"time_per_epoch\", logs.get(\"time\"), step=epoch)\n",
    "\n",
    "            # CPU and GPU Usage\n",
    "            tf.summary.scalar(\"cpu_usage\", logs.get(\"cpu_usage\"), step=epoch)\n",
    "\n",
    "            # Histograms of trainable variables\n",
    "            for var in self.model.trainable_variables:\n",
    "                tf.summary.histogram(var.name, var, step=epoch)\n",
    "\n",
    "            self.writer.flush()\n",
    "\n",
    "        if epoch == 0:  # Write the graph at the end of the first epoch\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.trace_export(name=\"model_trace\", step=epoch, profiler_outdir=self.tensorboard_logdir)\n",
    "        \n",
    "        # Compute and log the metrics every certain number of epochs\n",
    "        if epoch % 1 == 0:  # for example, every 10 epochs\n",
    "            \n",
    "            for i in range(self.num_samples):\n",
    "                img_path = f\"output_molecules_model2_zinc15_ii/molecule_epoch_{epoch}_image_{i}.png\"\n",
    "\n",
    "                # Only proceed if the image file exists\n",
    "                if os.path.exists(img_path):\n",
    "                    # Load the image file\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "                    # Add an extra dimension (for the batch), and scale to [0, 1]\n",
    "                    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "\n",
    "                    # Log the image to TensorBoard\n",
    "                    with self.writer.as_default():\n",
    "                        tf.summary.image(f\"generated_graph_{i}\", img_array, step=epoch)\n",
    "\n",
    "            molecules, none_counter = self.mol_sample(self.model.generator, batch_size=100)\n",
    "            # Calculate the percentage of None molecules\n",
    "            none_percentage = none_counter / 100  # Assuming batch_size is 100\n",
    "            #valid_molecules = [m for m in molecules if m is not None]\n",
    "            valid_molecules = []\n",
    "            for m in molecules:\n",
    "                if m is not None and m.GetNumAtoms() > 0:  # add check for number of atoms\n",
    "                    mol_block = Chem.MolToMolBlock(m)\n",
    "                    if not all(char == '0' for char in mol_block):\n",
    "                        valid_molecules.append(m)\n",
    "\n",
    "            connected_valid_molecules = [m for m in valid_molecules if len(GetMolFrags(m)) == 1]\n",
    "            connected_valid_molecules_with_Hs = [AddHs(mol) for mol in connected_valid_molecules]\n",
    "\n",
    "            quinoline_scaffold = Chem.MolFromSmiles(\"n1cccc2ccccc12\")\n",
    "            similarities = []\n",
    "\n",
    "            for m in valid_molecules:\n",
    "                if m.GetNumAtoms() > 0:  # add check for number of atoms\n",
    "                    try:\n",
    "                        mol_block = Chem.MolToMolBlock(m)\n",
    "                        # Get the Bemis-Murcko scaffold\n",
    "                        scaffold = MurckoScaffold.GetScaffoldForMol(m)\n",
    "\n",
    "                        # Compute the Tanimoto similarity\n",
    "                        #similarity = DataStructs.TanimotoSimilarity(Chem.RDKFingerprint(quinoline_scaffold), Chem.RDKFingerprint(scaffold))\n",
    "                        similarity = DataStructs.TanimotoSimilarity(AllChem.GetMorganFingerprint(quinoline_scaffold, 2), AllChem.GetMorganFingerprint(scaffold, 2))\n",
    "\n",
    "                        # Add the total reward to the list of rewards\n",
    "                        similarities.append(similarity)\n",
    "                    except Chem.rdchem.AtomValenceException:\n",
    "                        print(\"Invalid molecule skipped due to AtomValenceException\")\n",
    "\n",
    "            average_similarity = 0\n",
    "\n",
    "            if len(valid_molecules) > 0:\n",
    "                validity = len(valid_molecules) / len(molecules)\n",
    "\n",
    "                if len(connected_valid_molecules) > 0:\n",
    "                    connected_validity = len(connected_valid_molecules_with_Hs) / len(molecules)\n",
    "                else:\n",
    "                    connected_validity = 0\n",
    "\n",
    "                unique_molecules = list(set(valid_molecules))\n",
    "                if len(unique_molecules) > 0:\n",
    "                    uniqueness = len(unique_molecules) / len(valid_molecules)\n",
    "\n",
    "                    novel_molecules = [Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False) for mol in unique_molecules if Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False) not in self.original_smiles]\n",
    "                    if len(novel_molecules) > 0:\n",
    "                        novelty = len(novel_molecules) / len(unique_molecules)\n",
    "                    else:\n",
    "                        novelty = 0\n",
    "\n",
    "                    # Calculate absolute novelty\n",
    "                    absolute_novel_molecules = [Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False) for mol in unique_molecules if Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False) not in self.full_quinolines_smiles]\n",
    "                    if len(absolute_novel_molecules) > 0:\n",
    "                        absolute_novelty = len(absolute_novel_molecules) / len(unique_molecules)\n",
    "                    else:\n",
    "                        absolute_novelty = 0\n",
    "\n",
    "                else:\n",
    "                    uniqueness = 0\n",
    "                    novelty = 0\n",
    "                    absolute_novelty = 0\n",
    "            else:\n",
    "                validity = 0\n",
    "                novelty = 0\n",
    "                uniqueness = 0\n",
    "                connected_validity = 0\n",
    "                absolute_novelty = 0\n",
    "\n",
    "            if len(valid_molecules) > 0:\n",
    "                quinoline_molecules = [mol for mol in valid_molecules if mol.HasSubstructMatch(self.quinoline_scaffold)]\n",
    "                quinoline_percentage = len(quinoline_molecules) / len(valid_molecules)\n",
    "            else:\n",
    "                quinoline_percentage = 0\n",
    "\n",
    "            if validity > 0:\n",
    "                # Create a new directory for the current epoch\n",
    "                epoch_dir = os.path.join(\"output_molecules\", f\"epoch_{epoch}\")\n",
    "                os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "                # Save up to five molecule images\n",
    "                for i, mol in enumerate(valid_molecules):   # for i, mol in enumerate(valid_molecules[:5]):\n",
    "                    \n",
    "                    # Save the molecule as a MOL file\n",
    "                    mol_path = os.path.join(epoch_dir, f\"molecule_{i}.mol\")\n",
    "                    mol_block = Chem.MolToMolBlock(mol)\n",
    "                    \n",
    "                    # Check if the mol file only contains zeros\n",
    "                    if not all(char == '0' for char in mol_block):\n",
    "                        # Save the image\n",
    "                        img_path = os.path.join(epoch_dir, f\"molecule_image_{i}.png\")\n",
    "                        Draw.MolToFile(mol, img_path, size=(300, 300))\n",
    "\n",
    "                        # Save the MOL file\n",
    "                        rdmolfiles.MolToMolFile(mol, mol_path)\n",
    "\n",
    "            if len(similarities) > 0:\n",
    "                average_similarity = sum(similarities) / len(similarities) if similarities else 0\n",
    "            else:\n",
    "                average_similarity = 0\n",
    "\n",
    "            # Log the metrics\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\"validity\", validity, step=epoch)\n",
    "                tf.summary.scalar(\"connected_validity\", connected_validity, step=epoch)\n",
    "                tf.summary.scalar(\"uniqueness\", uniqueness, step=epoch)\n",
    "                tf.summary.scalar(\"novelty\", novelty, step=epoch)\n",
    "                tf.summary.scalar(\"absolute_novelty\", absolute_novelty, step=epoch)\n",
    "                tf.summary.scalar(\"quinoline_percentage\", quinoline_percentage, step=epoch)\n",
    "                tf.summary.scalar(\"average_tanimoto_similarity\", average_similarity if average_similarity != 0 else 0, step=epoch)\n",
    "                tf.summary.scalar(\"none_molecule_percentage\", none_percentage, step=epoch)\n",
    "                self.writer.flush()\n",
    "\n",
    "            logs['quinoline_percentage'] = quinoline_percentage           \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the following code, the WGAN is constructed and trained. The PlotSamplesCallback class is a custom Keras callback to visualize and plot samples at the end of each training epoch. This helps in monitoring the evolution of generated structures over the training process. To ensure continuity and robustness in training, checkpoints are used. A data_generator function is defined to produce training batches from provided tensors, ensuring that the data is fed correctly into the GAN during training. The GAN is then trained using the fit method, with checkpoints, sample plotting, and logging utilities as its callbacks. After training is complete, the model is saved, providing a reusable pre-trained model for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotSamplesCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan, num_samples=1):\n",
    "        super().__init__()\n",
    "        self.wgan = wgan\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"\\nGenerating and plotting samples at epoch\", epoch)\n",
    "            self.wgan.plot_generated_graph(self.num_samples, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_mol = MolFromSmiles(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = \"logs_model2_zinc15_ii/WGAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GraphWGAN instance and configure it\n",
    "wgan = GraphWGAN(generator, discriminator, discriminator_steps=1)\n",
    "\n",
    "wgan.compile(\n",
    "    optimizer_generator=keras.optimizers.RMSprop(1e-4),\n",
    "    optimizer_discriminator=keras.optimizers.RMSprop(1e-4)\n",
    ")\n",
    "\n",
    "# Checkpoint configurations\n",
    "checkpoint_dir = 'training_checkpoints_model2_zinc15_ii/WGAN'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    epoch=tf.Variable(0),\n",
    "    generator=wgan.generator,\n",
    "    discriminator=wgan.discriminator,\n",
    "    optimizer_generator=wgan.optimizer_generator,\n",
    "    optimizer_discriminator=wgan.optimizer_discriminator,\n",
    ")\n",
    "\n",
    "def load_latest_checkpoint(checkpoint):\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Loading checkpoint from {latest_checkpoint}\")\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "        last_epoch = int(checkpoint.epoch.numpy())\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "        last_epoch = 0\n",
    "    return last_epoch\n",
    "\n",
    "# Load the latest checkpoint and get the starting epoch\n",
    "starting_epoch = load_latest_checkpoint(checkpoint)\n",
    "\n",
    "# Create a PlotSamplesCallback instance\n",
    "plot_samples_callback = PlotSamplesCallback(wgan, num_samples=1)\n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Update the checkpoint's epoch\n",
    "        checkpoint.epoch.assign(epoch + 1)\n",
    "        # Save the checkpoint at the end of each epoch\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        print(f\"Saving checkpoint for epoch {epoch + 1}\")\n",
    "\n",
    "checkpoint_callback = CustomModelCheckpoint()\n",
    "\n",
    "# Instantiate the GANLogger\n",
    "gan_logger = GANLogger(\n",
    "    tensorboard_logdir, \n",
    "    data['smiles'].tolist(), \n",
    "    full_quinolines_smiles, \n",
    "    num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(adjacency_tensor, feature_tensor, batch_size):\n",
    "    dataset_size = len(adjacency_tensor)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        # Shuffle indices at the start of each epoch\n",
    "        #np.random.shuffle(indices)\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_indices = indices[i: min(i + batch_size, dataset_size)]\n",
    "            batch_adjacency_tensor = adjacency_tensor[batch_indices]\n",
    "            batch_feature_tensor = feature_tensor[batch_indices]\n",
    "            yield [batch_adjacency_tensor, batch_feature_tensor]\n",
    "\n",
    "batch_size = 128  # Set batch size\n",
    "data_gen = data_generator(adjacency_tensor, feature_tensor, batch_size)\n",
    "\n",
    "steps_per_epoch = len(adjacency_tensor) // batch_size\n",
    "if len(adjacency_tensor) % batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# Train the model\n",
    "wgan.fit(\n",
    "    data_gen,\n",
    "    initial_epoch=starting_epoch,\n",
    "    epochs=300,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[\n",
    "        checkpoint_callback, \n",
    "        plot_samples_callback, \n",
    "        gan_logger, \n",
    "        ],\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "wgan.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
