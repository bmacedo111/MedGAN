{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Drug-Likeness\n",
    "\n",
    "This code allows to train a model for toxicology, using the Tox21 dataset and pre-trained model, filtered for molecules up to 50 atoms and with atom types C,H,N,O,Cl,F,S. Also allows to address the Lipinski Rule of 5 and Synthetic Accessibility, usil Erl algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate molecules from trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to generate molecules and create them in the folders of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Toxicology model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepchem as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from deepchem.data import DiskDataset\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Get the dataset\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='Raw')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "# Define a function to filter the molecules\n",
    "def filter_molecules(dataset, max_atoms=50, allowed_atoms=set(['C','H','N','O','F','S','Cl'])):\n",
    "    valid_inds = []\n",
    "    for i in range(dataset.X.shape[0]):\n",
    "        molecule = Chem.MolFromSmiles(dataset.ids[i])\n",
    "        if molecule.GetNumAtoms() <= max_atoms:\n",
    "            atoms = [atom.GetSymbol() for atom in molecule.GetAtoms()]\n",
    "            if set(atoms).issubset(allowed_atoms):\n",
    "                valid_inds.append(i)\n",
    "    return dataset.select(valid_inds)\n",
    "\n",
    "# Apply the filter to all datasets\n",
    "train_dataset = filter_molecules(train_dataset)\n",
    "valid_dataset = filter_molecules(valid_dataset)\n",
    "test_dataset = filter_molecules(test_dataset)\n",
    "\n",
    "# Print the number of molecules after filtering\n",
    "print(f\"Number of molecules in training dataset after filtering: {len(train_dataset)}\")\n",
    "print(f\"Number of molecules in validation dataset after filtering: {len(valid_dataset)}\")\n",
    "print(f\"Number of molecules in test dataset after filtering: {len(test_dataset)}\")\n",
    "\n",
    "featurizer = dc.feat.ConvMolFeaturizer()\n",
    "\n",
    "train_mols = [Chem.MolToSmiles(mol) for mol in train_dataset.X]\n",
    "valid_mols = [Chem.MolToSmiles(mol) for mol in valid_dataset.X]\n",
    "test_mols = [Chem.MolToSmiles(mol) for mol in test_dataset.X]\n",
    "\n",
    "train_features = featurizer.featurize(train_mols)\n",
    "valid_features = featurizer.featurize(valid_mols)\n",
    "test_features = featurizer.featurize(test_mols)\n",
    "\n",
    "# Create DiskDataset from the features\n",
    "train_dataset = DiskDataset.from_numpy(train_features, train_dataset.y, train_dataset.w, ids=train_dataset.ids)\n",
    "valid_dataset = DiskDataset.from_numpy(valid_features, valid_dataset.y, valid_dataset.w, ids=valid_dataset.ids)\n",
    "test_dataset = DiskDataset.from_numpy(test_features, test_dataset.y, test_dataset.w, ids=test_dataset.ids)\n",
    "\n",
    "# Initialize the model\n",
    "model = dc.models.GraphConvModel(n_tasks=len(tasks), mode='classification')\n",
    "\n",
    "# Lists to store the performance at each epoch\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "\n",
    "# Directory to save or load the model\n",
    "model_dir = \"toxicity_model\"\n",
    "\n",
    "# Check if a checkpoint exists\n",
    "if os.path.exists(model_dir) and os.listdir(model_dir):\n",
    "    # Restore the model from the checkpoint\n",
    "    model.restore(model_dir=model_dir)\n",
    "    print(\"Model loaded from checkpoint.\")\n",
    "else:\n",
    "    # If no checkpoint exists, train the model from scratch\n",
    "    metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "\n",
    "    # Number of epochs\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = model.fit(train_dataset, nb_epoch=1)\n",
    "        \n",
    "        train_score = model.evaluate(train_dataset, [metric], transformers)\n",
    "        valid_score = model.evaluate(valid_dataset, [metric], transformers)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        valid_scores.append(valid_score)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}, Train AUC: {train_score['mean-roc_auc_score']}, Valid AUC: {valid_score['mean-roc_auc_score']}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save_checkpoint(max_checkpoints_to_keep=100, model_dir=model_dir)\n",
    "\n",
    "    # Plot the performance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(num_epochs)+1, [x['mean-roc_auc_score'] for x in train_scores], label='Training')\n",
    "    plt.plot(np.arange(num_epochs)+1, [x['mean-roc_auc_score'] for x in valid_scores], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean ROC AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"performance_plot.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate on the test set\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "print(model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import Chem\n",
    "import deepchem as dc\n",
    "\n",
    "# List of directories containing the .mol files\n",
    "mol_dirs = [\n",
    "    \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "]\n",
    "\n",
    "# Define target names in the correct order\n",
    "target_names = [\n",
    "    \"NR-AhR\", \"NR-AR\", \"NR-AR-LBD\", \"NR-Aromatase\", \"NR-ER\",\n",
    "    \"NR-ER-LBD\", \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\",\n",
    "    \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n",
    "]\n",
    "\n",
    "# Featurizer used in training\n",
    "featurizer = dc.feat.ConvMolFeaturizer()\n",
    "\n",
    "# Iterate through each model directory\n",
    "for mol_dir in mol_dirs:\n",
    "\n",
    "    # Extract the model name from the directory path\n",
    "    model_name = os.path.basename(os.path.normpath(mol_dir))\n",
    "\n",
    "    # Create a directory to save the CSV file for this model\n",
    "    output_dir = os.path.join(mol_dir, \"predicted_toxicity\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List to store RDKit Mol objects and their filenames\n",
    "    molecules_mols = []\n",
    "    filenames = []\n",
    "\n",
    "    # Get the list of .mol files\n",
    "    mol_files = [f for f in os.listdir(mol_dir) if f.endswith('.mol')]\n",
    "\n",
    "    # Iterate through the .mol files in the directory with progress bar\n",
    "    for mol_file in tqdm(mol_files, desc=f\"Processing .mol files in {model_name}\"):\n",
    "        mol_path = os.path.join(mol_dir, mol_file)\n",
    "        mol = Chem.MolFromMolFile(mol_path)\n",
    "        if mol is not None:  # Check if the molecule was read successfully\n",
    "            molecules_mols.append(mol)\n",
    "            filenames.append(mol_file)\n",
    "\n",
    "    # Featurize the RDKit Mol objects\n",
    "    molecules_features = featurizer.featurize(molecules_mols)\n",
    "\n",
    "    # Create a DiskDataset from the features\n",
    "    molecules_dataset = dc.data.DiskDataset.from_numpy(molecules_features, ids=filenames)\n",
    "\n",
    "    # Predict toxicity using the trained model\n",
    "    predictions = model.predict(molecules_dataset)\n",
    "\n",
    "    # Create a DataFrame with predictions for this model\n",
    "    model_predictions = pd.DataFrame(predictions[:, :, 1], columns=target_names)\n",
    "    model_predictions['Molecule_File'] = filenames\n",
    "\n",
    "    # Save the DataFrame to a CSV file inside the model folder\n",
    "    output_csv = os.path.join(output_dir, f\"{model_name}_predicted_toxicity.csv\")\n",
    "    model_predictions.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dictionary containing model names\n",
    "model_names = {\n",
    "    \"model2_zinc15ii\": \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15ii\": \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15iii\": \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "}\n",
    "\n",
    "# Loop through each model directory\n",
    "for model_name, mol_dir in model_names.items():\n",
    "    # Extract the model name from the directory path (derived from the way you provided earlier)\n",
    "    derived_model_name = os.path.basename(os.path.normpath(mol_dir))\n",
    "    \n",
    "    # Define the path to the saved CSV file\n",
    "    csv_path = os.path.join(mol_dir, \"predicted_toxicity\", f\"{derived_model_name}_predicted_toxicity.csv\")\n",
    "    \n",
    "    # Check if the CSV file exists\n",
    "    if os.path.exists(csv_path):\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Print the model name (from the dictionary) and the first 10 rows of the DataFrame\n",
    "        print(f\"Predictions for {model_name}:\\n\")\n",
    "        print(df.head(10))\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")  # Print a separator line for better readability\n",
    "    else:\n",
    "        print(f\"No prediction CSV found for {model_name}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary containing model names and their corresponding directories\n",
    "model_to_dir = {\n",
    "    \"model2_zinc15ii\": \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15ii\": \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15iii\": \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "}\n",
    "\n",
    "# Define target names in the correct order (assuming you've already defined it)\n",
    "target_names = [\n",
    "    \"NR-AhR\", \"NR-AR\", \"NR-AR-LBD\", \"NR-Aromatase\", \"NR-ER\",\n",
    "    \"NR-ER-LBD\", \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\",\n",
    "    \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n",
    "]\n",
    "\n",
    "# Define a threshold for classifying as toxic or non-toxic\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each model\n",
    "dfs = []\n",
    "\n",
    "# Load and append predictions for each model\n",
    "for model_name, mol_dir in model_to_dir.items():\n",
    "    csv_path = os.path.join(mol_dir, \"predicted_toxicity\", \"generated_molecules_predicted_toxicity.csv\")\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Model'] = model_name  # Add a 'Model' column to specify the model's name\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all loaded DataFrames\n",
    "final_predictions = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Loop through each model's name to compute and print active and inactive counts\n",
    "for model_name in model_to_dir.keys():\n",
    "    # Filter the predictions for the current model\n",
    "    model_predictions = final_predictions[final_predictions['Model'] == model_name]\n",
    "    \n",
    "    print(f\"\\nStatistics for {model_name}:\\n\")\n",
    "    \n",
    "    # Create empty dictionaries to store counts for the current model\n",
    "    active_counts = {target: 0 for target in target_names}\n",
    "    inactive_counts = {target: 0 for target in target_names}\n",
    "\n",
    "    # Calculate mean toxicity for each target and count active/inactive\n",
    "    for target in target_names:\n",
    "        mean_toxicity = model_predictions[target].mean()\n",
    "        print(f\"Mean toxicity for {target}: {mean_toxicity:.2f}\")\n",
    "\n",
    "        # Count active and inactive molecules for the target\n",
    "        active_counts[target] = (model_predictions[target] >= threshold).sum()\n",
    "        inactive_counts[target] = (model_predictions[target] < threshold).sum()\n",
    "\n",
    "    # Print the frequency of active and inactive targets\n",
    "    print(\"\\nActive counts:\", active_counts)\n",
    "    print(\"Inactive counts:\", inactive_counts)\n",
    "    print(\"-\" * 50)  # Line separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dictionary containing model names and their corresponding directories\n",
    "model_to_dir = {\n",
    "    \"model2_zinc15ii\": \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15ii\": \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"model3_zinc15iii\": \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "}\n",
    "\n",
    "# Extract model names to a separate list\n",
    "model_names = list(model_to_dir.keys())\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each model\n",
    "dfs = []\n",
    "\n",
    "# Load and append predictions for each model\n",
    "for model_name, mol_dir in model_to_dir.items():\n",
    "    csv_path = os.path.join(mol_dir, \"predicted_toxicity\", \"generated_molecules_predicted_toxicity.csv\")\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Model'] = model_name  # Add a 'Model' column to specify the model's name\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all loaded DataFrames\n",
    "final_predictions = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Loop through each model name\n",
    "for model_name in model_names:\n",
    "    mol_dir = model_to_dir[model_name]\n",
    "    \n",
    "    model_predictions = final_predictions[final_predictions['Model'] == model_name]\n",
    "    print(model_name, model_predictions.shape)\n",
    "\n",
    "    # Calculate the frequency of active and inactive predictions for each target\n",
    "    active_counts = {target: (model_predictions[target] > 0.5).sum() for target in target_names}\n",
    "    inactive_counts = {target: (model_predictions[target] <= 0.5).sum() for target in target_names}\n",
    "\n",
    "    # Calculate how many compounds are inactive and active for all the targets\n",
    "    all_inactive_count = (model_predictions[target_names] <= 0.5).all(axis=1).sum()\n",
    "    all_active_count = (model_predictions[target_names] > 0.5).all(axis=1).sum()\n",
    "    active_counts['all'] = all_active_count\n",
    "    inactive_counts['all'] = all_inactive_count\n",
    "\n",
    "    # Convert active and inactive counts to lists\n",
    "    active_values = list(active_counts.values())\n",
    "    inactive_values = list(inactive_counts.values())\n",
    "\n",
    "    # Extend the target names with the \"all\" category\n",
    "    target_names_with_all = target_names + ['all']\n",
    "\n",
    "    # Set up the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    # Set the bar positions\n",
    "    bar_width = 0.35\n",
    "    index = range(len(target_names_with_all))\n",
    "\n",
    "    # Plot the bars for active and inactive counts\n",
    "    bar1 = plt.bar(index, active_values, bar_width, label=\"Active\")\n",
    "    bar2 = plt.bar([i + bar_width for i in index], inactive_values, bar_width, label=\"Inactive\")\n",
    "\n",
    "    # Add some text for labels, title and axes ticks\n",
    "    plt.xlabel('Targets')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency of Active and Inactive Predictions for {model_name}')\n",
    "    plt.xticks([i + bar_width/2 for i in index], target_names_with_all, rotation=90)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# List of directories containing the .mol files\n",
    "mol_dirs = [\n",
    "    \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "]\n",
    "\n",
    "master_data = []\n",
    "\n",
    "# Initialize an empty list to store DataFrames from each model\n",
    "dfs = []\n",
    "\n",
    "# Load and append predictions for each model\n",
    "for mol_dir in mol_dirs:\n",
    "    csv_path = os.path.join(mol_dir, \"predicted_toxicity\", \"generated_molecules_predicted_toxicity.csv\")\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        model_name = os.path.basename(os.path.dirname(os.path.dirname(mol_dir)))  # Extracting the model name from the grandparent directory path\n",
    "        df['Model'] = model_name  # Add a 'Model' column to specify the model's name\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all loaded DataFrames\n",
    "final_predictions = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Loop through each model's directory\n",
    "for mol_dir in mol_dirs:\n",
    "    model_name = os.path.basename(os.path.dirname(os.path.dirname(mol_dir)))  # Extracting the model name from the grandparent directory path\n",
    "    model_predictions = final_predictions[final_predictions['Model'] == model_name]\n",
    "        \n",
    "    # Calculate mean and median toxicity for each target\n",
    "    mean_values = {target: model_predictions[target].mean() for target in target_names}\n",
    "    median_values = {target: model_predictions[target].median() for target in target_names}\n",
    "\n",
    "    # Calculate mean and median for \"all\" category\n",
    "    mean_values['all'] = model_predictions[target_names].mean(axis=1).mean()\n",
    "    median_values['all'] = model_predictions[target_names].mean(axis=1).median()\n",
    "\n",
    "    # Calculate active and inactive counts for the model\n",
    "    active_counts = {target: (model_predictions[target] > 0.5).sum() for target in target_names}\n",
    "    inactive_counts = {target: (model_predictions[target] <= 0.5).sum() for target in target_names}\n",
    "    all_inactive_count = (model_predictions[target_names] <= 0.5).all(axis=1).sum()\n",
    "    all_active_count = (model_predictions[target_names] > 0.5).all(axis=1).sum()\n",
    "    active_counts['all'] = all_active_count\n",
    "    inactive_counts['all'] = all_inactive_count\n",
    "\n",
    "    # Prepare the table\n",
    "    table_data = []\n",
    "    target_names_with_all = target_names + ['all']\n",
    "    for target in target_names_with_all:\n",
    "        table_data.append([model_name, target, mean_values[target], median_values[target], active_counts[target], inactive_counts[target]])\n",
    "        master_data.append([model_name, target, mean_values[target], median_values[target], active_counts[target], inactive_counts[target]])\n",
    "\n",
    "    # Add headers\n",
    "    headers = ['Model', 'Target', 'Mean Toxicity', 'Median Toxicity', 'Active Count', 'Inactive Count']\n",
    "\n",
    "    # Print the table\n",
    "    print(tabulate(table_data, headers=headers))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert master_data to DataFrame and save to CSV\n",
    "df_master = pd.DataFrame(master_data, columns=headers)\n",
    "df_master.to_csv(\"model_statistics.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess lipinski rule of five and synthetic accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Lipinski, Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit import Chem\n",
    "import sascorer\n",
    "import os\n",
    "\n",
    "def lipinski_rule_of_five(molecule):\n",
    "    mw = Descriptors.MolWt(molecule)\n",
    "    logp = Descriptors.MolLogP(molecule)\n",
    "    hbd = Lipinski.NumHDonors(molecule)\n",
    "    hba = Lipinski.NumHAcceptors(molecule)\n",
    "    return mw <= 500 and logp <= 5 and hbd <= 5 and hba <= 10\n",
    "\n",
    "def synthetic_accessibility(molecule):\n",
    "    sas = sascorer.calculateScore(molecule)\n",
    "    return sas\n",
    "\n",
    "# List of directories containing the .mol files\n",
    "mol_dirs = [\n",
    "    \"training_models/model2_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15ii/generated_molecules/\",\n",
    "    \"training_models/model3_zinc15iii/generated_molecules/\"\n",
    "]\n",
    "\n",
    "for mol_dir in mol_dirs:\n",
    "    model_name = os.path.basename(os.path.dirname(os.path.dirname(mol_dir)))  # Extracting the model name from the grandparent directory path\n",
    "    mol_files = [f for f in os.listdir(mol_dir) if f.endswith('.mol')]\n",
    "    \n",
    "    molecules_mols = []\n",
    "    filenames = []\n",
    "\n",
    "    for mol_file in mol_files:\n",
    "        mol_path = os.path.join(mol_dir, mol_file)\n",
    "        mol = Chem.MolFromMolFile(mol_path)\n",
    "        if mol is not None:  # Check if the molecule was read successfully\n",
    "            molecules_mols.append(mol)\n",
    "            filenames.append(mol_file)\n",
    "\n",
    "    # Calculate Lipinski and SAS for the molecules from the current model\n",
    "    lipinski_results = [lipinski_rule_of_five(mol) for mol in molecules_mols]\n",
    "    sas_results = [synthetic_accessibility(mol) for mol in molecules_mols]\n",
    "\n",
    "    # Update the final_predictions DataFrame\n",
    "    model_mask = final_predictions['Model'] == model_name\n",
    "    final_predictions.loc[model_mask, 'Lipinski_Rule_of_Five'] = lipinski_results\n",
    "    final_predictions.loc[model_mask, 'Synthetic_Accessibility'] = sas_results\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "final_predictions.to_csv(\"predicted_toxicity_with_properties.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split `final_predictions` dataframe into separate dataframes for each model\n",
    "all_predictions = [final_predictions[final_predictions['Model'] == model] for model in final_predictions['Model'].unique()]\n",
    "\n",
    "# Then, you can use the loop as before:\n",
    "for idx, df in enumerate(all_predictions):\n",
    "    model_name = os.path.basename(os.path.dirname(os.path.dirname(mol_dirs[idx])))  # Extracting the model name from the grandparent directory path\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(df.head(10))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# This dictionary will help map the 'mol_dir' to its respective model name.\n",
    "model_name_dict = {mol_dir: os.path.basename(os.path.dirname(os.path.dirname(mol_dir))) for mol_dir in mol_dirs}\n",
    "\n",
    "# Assuming 'all_sas_results' is a list of sas_results for each model\n",
    "all_sas_results = []  # This needs to be populated with data before running the loop\n",
    "\n",
    "# Add this part to populate all_sas_results from the DataFrame you saved\n",
    "final_predictions = pd.read_csv(\"predicted_toxicity_with_properties.csv\")\n",
    "for mol_dir in mol_dirs:\n",
    "    model_name = model_name_dict[mol_dir]  # Use the model_name_dict here\n",
    "    all_sas_results.append(final_predictions.loc[final_predictions['Model'] == model_name, 'Synthetic_Accessibility'].tolist())\n",
    "\n",
    "ranges_colors = [((0, 0.99), 'red'), ((1, 3), 'blue'), ((3.01, 6), 'green'), ((6.01, 8), 'yellow'), ((8.01, 10), 'purple')]\n",
    "\n",
    "for idx, sas_results in enumerate(all_sas_results):\n",
    "    model_name = model_name_dict[mol_dirs[idx]]  # Use the model_name_dict here as well\n",
    "    print(f\"Processing Model: {model_name}\")\n",
    "    \n",
    "    save_directory = os.path.join(mol_dirs[idx], \"plot_results\")  # This now creates the 'plot_results' directory inside each model's directory\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Define the new ranges\n",
    "    ranges = [(0, 0.99), (1, 3), (3.01, 6), (6.01, 8), (8.01, 10)]\n",
    "    frequency = {f\"{r[0]}-{r[1]}\": 0 for r in ranges}\n",
    "\n",
    "    # Calculate the frequency\n",
    "    for score in sas_results:\n",
    "        for r in ranges:\n",
    "            if r[0] <= score <= r[1]:\n",
    "                frequency[f\"{r[0]}-{r[1]}\"] += 1\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.figure(figsize=[10,6])\n",
    "    plt.hist(sas_results, bins=[0, 1, 3.01, 6.01, 8.01, 10], edgecolor='black')\n",
    "    plt.xlabel('SA Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency of SA Scores for {model_name}')\n",
    "    plt.xticks([i + 0.5 for i in [0, 1, 3.01, 6.01, 8.01]], [f\"{r[0]}-{r[1]}\" for r in ranges])\n",
    "\n",
    "    # Save the histogram\n",
    "    plt.savefig(os.path.join(save_directory, f\"{model_name}_histogram.png\"))\n",
    "\n",
    "    # Print the tabulate table\n",
    "    table_data = [[key, value] for key, value in frequency.items()]\n",
    "    headers = ['SA Score Range', 'Frequency']\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=[10,6])\n",
    "    for i, score in enumerate(sas_results):\n",
    "        for r, color in ranges_colors:\n",
    "            if r[0] <= score <= r[1]:\n",
    "                plt.scatter(i, score, c=color)\n",
    "\n",
    "    plt.xlabel('Molecule Index')\n",
    "    plt.ylabel('SA Score')\n",
    "    plt.title(f'Synthetic Accessibility Scores for {model_name}')\n",
    "    plt.yticks([i + 1 for i in range(10)])\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Create a custom legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=f'{r[0]}-{r[1]}') for r, color in ranges_colors]\n",
    "    plt.legend(handles=legend_elements, title=\"SA Score Range\")\n",
    "\n",
    "    # Save the scatter plot\n",
    "    plt.savefig(os.path.join(save_directory, f\"{model_name}_scatter_plot.png\"))\n",
    "\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the data\n",
    "final_predictions = pd.read_csv(\"predicted_toxicity_with_properties.csv\")\n",
    "\n",
    "# Create a dictionary that maps each mol_dir to its respective model name.\n",
    "model_name_dict = {mol_dir: os.path.basename(os.path.dirname(os.path.dirname(mol_dir))) for mol_dir in mol_dirs}\n",
    "\n",
    "for mol_dir in mol_dirs:\n",
    "    # Use the model_name_dict to get the correct model name\n",
    "    model_name = model_name_dict[mol_dir]\n",
    "    print(f\"Model: {model_name}\")\n",
    "\n",
    "    # Filter the DataFrame for the current model\n",
    "    model_data = final_predictions[final_predictions['Model'] == model_name]\n",
    "\n",
    "    # Tally up the 'Pass' and 'Fail' results\n",
    "    pass_count = sum(model_data['Lipinski_Rule_of_Five'])\n",
    "    fail_count = len(model_data) - pass_count\n",
    "    \n",
    "    # Create a table\n",
    "    table_data_lipinski = [['Pass', pass_count], ['Fail', fail_count]]\n",
    "    headers_lipinski = ['Lipinski Rule of Five', 'Frequency']\n",
    "    print(tabulate(table_data_lipinski, headers=headers_lipinski, tablefmt='grid'))\n",
    "\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
